{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72a29143",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     15\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m1337\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv1D, MaxPooling1D, Embedding\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshlex\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msubprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen, PIPE  \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Mar  4 20:42:24 2017; Feb/2020\n",
    "\n",
    "@author: firojalam\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "# for reproducibility\n",
    "seed = 1337\n",
    "np.random.seed(seed)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "import shlex\n",
    "from subprocess import Popen, PIPE  \n",
    "from collections import Counter\n",
    "import random\n",
    "from keras.layers import concatenate\n",
    "from keras.constraints import max_norm\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, MaxPooling2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "\n",
    "def get_exitcode_stdout_stderr(cmd):\n",
    "    \"\"\"\n",
    "    Execute the external command and get its exitcode, stdout and stderr.\n",
    "    \"\"\"\n",
    "    args = shlex.split(cmd)\n",
    "\n",
    "    proc = Popen(args, stdout=PIPE, stderr=PIPE)\n",
    "    out, err = proc.communicate()\n",
    "    exitcode = proc.returncode\n",
    "    #\n",
    "    return exitcode, out, err\n",
    "    \n",
    "def label_one_hot(yL):\n",
    "    label=yL.tolist()\n",
    "    yC=len(set(label))\n",
    "    yR=len(label)\n",
    "    y = np.zeros((yR, yC))\n",
    "    y[np.arange(yR), yL] = 1\n",
    "    y=np.array(y,dtype=np.int32)\n",
    "    return y  \n",
    "    \n",
    "def upsampling(train_x,train_y):\n",
    "    ########## Upsampling    \n",
    "    y_true=np.argmax(train_y, axis = 1)\n",
    "    smote = \"\"#SMOTE(ratio=0.5, kind='borderline1',n_jobs=5)\n",
    "    X_resampled, y_resampled = smote.fit_sample(train_x,y_true)\n",
    "  \n",
    "    ########## Shuffling  \n",
    "    combined = list(zip(X_resampled, y_resampled))\n",
    "    random.shuffle(combined)\n",
    "    X_resampled[:], y_resampled[:] = zip(*combined)\n",
    "    y_resampled_true=label_one_hot(y_resampled)\n",
    "    dimension = X_resampled.shape[1]\n",
    "    y_resampled_true=label_one_hot(y_resampled)\n",
    "    print(len(X_resampled))\n",
    "    X_resampled=np.array(X_resampled)\n",
    "    print(X_resampled.shape)    \n",
    "    counts = Counter(y_resampled)\n",
    "    print(counts)   \n",
    "    return X_resampled, y_resampled_true, dimension\n",
    "  \n",
    "def text_cnn(embedding_matrix,word_index,MAX_NB_WORDS,EMBEDDING_DIM,MAX_SEQUENCE_LENGTH,inputs):\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
    "    embedding_layer=Embedding(output_dim=EMBEDDING_DIM, input_dim=nb_words, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,trainable=True)(inputs)\n",
    "    # embedding_layer=Embedding(output_dim=EMBEDDING_DIM, input_dim=nb_words, input_length=MAX_SEQUENCE_LENGTH,trainable=False)(inputs)\n",
    "\n",
    "    ########## CNN: Filtering with Max pooling:\n",
    "    #nb_filter = 250\n",
    "    #filter_length = 3\n",
    "    branches = [] # models to be merged\n",
    "    filter_window_sizes=[2,3,4,5]\n",
    "    pool_size=2\n",
    "    num_filters=[100,150,200,300]\n",
    "    for filter_len,nb_filter in zip(filter_window_sizes,num_filters):\n",
    "        branch = embedding_layer\n",
    "        branch=Conv1D(filters=nb_filter,\n",
    "                                 kernel_size=int(filter_len),\n",
    "                                 padding='valid',\n",
    "                                 activation='relu',\n",
    "                                 strides=1,\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 kernel_constraint=max_norm(3), bias_constraint=max_norm(3))(branch)\n",
    "        branch=MaxPooling1D(pool_size=pool_size)(branch)\n",
    "        branch=Flatten()(branch)\n",
    "        branches.append(branch)\n",
    "    merged_model=concatenate(branches)\n",
    "\n",
    "    return merged_model\n",
    "\n",
    "\n",
    "def text_cnn_2d(embedding_matrix,word_index,MAX_NB_WORDS,EMBEDDING_DIM,MAX_SEQUENCE_LENGTH,inputs):\n",
    "    # this returns a tensor\n",
    "    print(\"Creating Model...\")\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
    "    embedding=Embedding(output_dim=EMBEDDING_DIM, input_dim=nb_words, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,trainable=True)(inputs)\n",
    "    # embedding = Embedding(input_dim=nb_words, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(inputs)\n",
    "\n",
    "    reshape = Reshape((MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,1))(embedding)\n",
    "    filter_window_sizes = [2, 3, 4]\n",
    "    # filter_sizes = [3, 4, 5]\n",
    "    num_filters = 512\n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_window_sizes[0], EMBEDDING_DIM), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_window_sizes[1], EMBEDDING_DIM), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(filter_window_sizes[2], EMBEDDING_DIM), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "    maxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_window_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "    maxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_window_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "    maxpool_2 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_window_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "    concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "    merged_model = Flatten()(concatenated_tensor)\n",
    "    return merged_model\n",
    "\n",
    "\n",
    "\n",
    "## sentence CNN by Y.Kim\n",
    "def kimCNN(embedding_matrix,word_index,MAX_NB_WORDS,EMBEDDING_DIM,MAX_SEQUENCE_LENGTH,sequence_input):\n",
    "    \"\"\"\n",
    "    Convolution neural network model for sentence classification.\n",
    "    Parameters\n",
    "    ----------\n",
    "    EMBEDDING_DIM: Dimension of the embedding space.\n",
    "    MAX_SEQUENCE_LENGTH: Maximum length of the sentence.\n",
    "    MAX_NB_WORDS: Maximum number of words in the vocabulary.\n",
    "    embeddings_index: A dict containing words and their embeddings.\n",
    "    word_index: A dict containing words and their indices.\n",
    "    labels_index: A dict containing the labels and their indices.\n",
    "    Returns\n",
    "    -------\n",
    "    compiled keras model\n",
    "    \"\"\"\n",
    "    print('Preparing embedding matrix.')\n",
    "    # num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "    # nb_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "    # embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    # for word, i in word_index.items():\n",
    "    #     if i >= MAX_NB_WORDS:\n",
    "    #         continue\n",
    "    #     embedding_vector = embeddings_index.get(word)\n",
    "    #     if embedding_vector is not None:\n",
    "    #         # words not found in embedding index will be all-zeros.\n",
    "    #         embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # embedding_layer = Embedding(nb_words,\n",
    "    #                             EMBEDDING_DIM,\n",
    "    #                             weights=[embedding_matrix],\n",
    "    #                             input_length=MAX_SEQUENCE_LENGTH,\n",
    "    #                             trainable=True)\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
    "    embedding_layer = Embedding(output_dim=EMBEDDING_DIM, input_dim=nb_words, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,trainable=True)\n",
    "\n",
    "\n",
    "\n",
    "    print('Training model.')\n",
    "\n",
    "    # sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    print(embedded_sequences.shape)\n",
    "\n",
    "\n",
    "    # add first conv filter\n",
    "    embedded_sequences = Reshape((MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, 1))(embedded_sequences)\n",
    "    x = Conv2D(300, (5, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    x = MaxPool2D((MAX_SEQUENCE_LENGTH - 5 + 1, 1))(x)\n",
    "\n",
    "\n",
    "    # add second conv filter.\n",
    "    y = Conv2D(300, (4, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    y = MaxPool2D((MAX_SEQUENCE_LENGTH - 4 + 1, 1))(y)\n",
    "\n",
    "\n",
    "    # add third conv filter.\n",
    "    z = Conv2D(300, (3, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    z = MaxPool2D((MAX_SEQUENCE_LENGTH - 3 + 1, 1))(z)\n",
    "\n",
    "    # add third conv filter.\n",
    "    z1 = Conv2D(300, (2, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    z1 = MaxPool2D((MAX_SEQUENCE_LENGTH - 2 + 1, 1))(z1)\n",
    "\n",
    "    # add third conv filter.\n",
    "    w1 = Conv2D(300, (1, EMBEDDING_DIM), activation='relu')(embedded_sequences)\n",
    "    w1 = MaxPool2D((MAX_SEQUENCE_LENGTH - 1 + 1, 1))(w1)\n",
    "    # concate the conv layers\n",
    "    # alpha = concatenate([x,y,z,z1])\n",
    "    alpha = concatenate([w1,z1,z,y])\n",
    "\n",
    "    # flatted the pooled features.\n",
    "    merged_model = Flatten()(alpha)\n",
    "\n",
    "    return merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "id": "486abb62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
